{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Programming in Scala \n",
    "\n",
    "## Instructors: Viktor Kincak and Aleksander Prokopek\n",
    "\n",
    "> # Week 2: Basic Task-Parallel Algorithms\n",
    "\n",
    "**Author:** [Ehsan M. Kermani](https://ca.linkedin.com/in/ehsanmkermani)\n",
    "\n",
    "Codes are available [here](https://github.com/axel22/parprog-snippets/tree/master/src/main/scala/lectures/algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun Oct 25 12:00:40 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Wed Oct 21 02:44:30 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Wed Oct 21 08:05:05 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load.ivy(\"com.storm-enroute\" %% \"scalameter-core\" % \"0.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.scalameter._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.scalameter._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mjava.util.concurrent._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.util.DynamicVariable\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mcommon\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "  Taken from [[https://github.com/axel22/parprog-snippets]]\n",
    "*/\n",
    "\n",
    "import java.util.concurrent._\n",
    "import scala.util.DynamicVariable\n",
    "\n",
    "object common {\n",
    "\n",
    "  val forkJoinPool = new ForkJoinPool\n",
    "\n",
    "  abstract class TaskScheduler {\n",
    "    def schedule[T](body: => T): ForkJoinTask[T]\n",
    "    def parallel[A, B](taskA: => A, taskB: => B): (A, B) = {\n",
    "      val right = task {\n",
    "        taskB\n",
    "      }\n",
    "      val left = taskA\n",
    "      (left, right.join())\n",
    "    }\n",
    "  }\n",
    "\n",
    "  class DefaultTaskScheduler extends TaskScheduler {\n",
    "    def schedule[T](body: => T): ForkJoinTask[T] = {\n",
    "      val t = new RecursiveTask[T] {\n",
    "        def compute = body\n",
    "      }\n",
    "      Thread.currentThread match {\n",
    "        case wt: ForkJoinWorkerThread =>\n",
    "          t.fork()\n",
    "        case _ =>\n",
    "          forkJoinPool.execute(t)\n",
    "      }\n",
    "      t\n",
    "    }\n",
    "  }\n",
    "\n",
    "  val scheduler =\n",
    "    new DynamicVariable[TaskScheduler](new DefaultTaskScheduler)\n",
    "\n",
    "  def task[T](body: => T): ForkJoinTask[T] = {\n",
    "    scheduler.value.schedule(body)\n",
    "  }\n",
    "\n",
    "  def parallel[A, B](taskA: => A, taskB: => B): (A, B) = {\n",
    "    scheduler.value.parallel(taskA, taskB)\n",
    "  }\n",
    "\n",
    "  def parallel[A, B, C, D](taskA: => A, taskB: => B, taskC: => C, taskD: => D): (A, B, C, D) = {\n",
    "    val ta = task { taskA }\n",
    "    val tb = task { taskB }\n",
    "    val tc = task { taskC }\n",
    "    val td = taskD\n",
    "    (ta.join(), tb.join(), tc.join(), td)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mcommon._\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mMergeSort\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "  Taken from [[https://github.com/axel22/parprog-snippets]]\n",
    "*/\n",
    "\n",
    "import common._\n",
    "\n",
    "object MergeSort {\n",
    "  // a bit of reflection to access the private sort1 method, which takes an offset and an argument\n",
    "  private val sort1 = {\n",
    "    val method = scala.util.Sorting.getClass.getDeclaredMethod(\"sort1\", classOf[Array[Int]], classOf[Int], classOf[Int])\n",
    "    method.setAccessible(true)\n",
    "    (xs: Array[Int], offset: Int, len: Int) => {\n",
    "      method.invoke(scala.util.Sorting, xs, offset.asInstanceOf[AnyRef], len.asInstanceOf[AnyRef])\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def quickSort(xs: Array[Int], offset: Int, length: Int): Unit = {\n",
    "    sort1(xs, offset, length)\n",
    "  }\n",
    "\n",
    "  @volatile var dummy: AnyRef = null\n",
    "\n",
    "  def parMergeSort(xs: Array[Int], maxDepth: Int): Unit = {\n",
    "    // 1) Allocate a helper array.\n",
    "    // This step is a bottleneck, and takes:\n",
    "    // - ~76x less time than a full quickSort without GCs (best time)\n",
    "    // - ~46x less time than a full quickSort with GCs (average time)\n",
    "    // Therefore:\n",
    "    // - there is a almost no performance gain in executing allocation concurrently to the sort\n",
    "    // - doing so would immensely complicate the algorithm\n",
    "    val ys = new Array[Int](xs.length)\n",
    "    dummy = ys\n",
    "    \n",
    "    // 2) Sort the elements.\n",
    "    // The merge step has to do some copying, and is the main performance bottleneck of the algorithm.\n",
    "    // This is due to the final merge call, which is a completely sequential pass.\n",
    "    def merge(src: Array[Int], dst: Array[Int], from: Int, mid: Int, until: Int) {\n",
    "      var left = from\n",
    "      var right = mid\n",
    "      var i = from\n",
    "      while (left < mid && right < until) {\n",
    "        while (left < mid && src(left) <= src(right)) {\n",
    "          dst(i) = src(left)\n",
    "          i += 1\n",
    "          left += 1\n",
    "        }\n",
    "        while (right < until && src(right) <= src(left)) {\n",
    "          dst(i) = src(right)\n",
    "          i += 1\n",
    "          right += 1\n",
    "        }\n",
    "      }\n",
    "      while (left < mid) {\n",
    "        dst(i) = src(left)\n",
    "        i += 1\n",
    "        left += 1\n",
    "      }\n",
    "      while (right < mid) {\n",
    "        dst(i) = src(right)\n",
    "        i += 1\n",
    "        right += 1\n",
    "      }\n",
    "    }\n",
    "    // Without the merge step, the sort phase parallelizes almost linearly.\n",
    "    // This is because the memory pressure is much lower than during copying in the third step.\n",
    "    def sort(from: Int, until: Int, depth: Int): Unit = {\n",
    "      if (depth == maxDepth) {\n",
    "        quickSort(xs, from, until - from)\n",
    "      } else {\n",
    "        val mid = (from + until) / 2\n",
    "        val right = task {\n",
    "          sort(mid, until, depth + 1)\n",
    "        }\n",
    "        sort(from, mid, depth + 1)\n",
    "        right.join()\n",
    "\n",
    "        val flip = (maxDepth - depth) % 2 == 0\n",
    "        val src = if (flip) ys else xs\n",
    "        val dst = if (flip) xs else ys\n",
    "        merge(src, dst, from, mid, until)\n",
    "      }\n",
    "    }\n",
    "    sort(0, xs.length, 0)\n",
    "\n",
    "    // 3) In parallel, copy the elements back into the source array.\n",
    "    // Executed sequentially, this step takes:\n",
    "    // - ~23x less time than a full quickSort without GCs (best time)\n",
    "    // - ~16x less time than a full quickSort with GCs (average time)\n",
    "    // There is a small potential gain in parallelizing copying.\n",
    "    // However, most Intel processors have a dual-channel memory controller,\n",
    "    // so parallel copying has very small performance benefits.\n",
    "    def copy(src: Array[Int], target: Array[Int], from: Int, until: Int, depth: Int): Unit = {\n",
    "      if (depth == maxDepth) {\n",
    "        Array.copy(src, from, target, from, until - from)\n",
    "      } else {\n",
    "        val mid = from + ((until - from) / 2)\n",
    "        val right = task {\n",
    "          copy(src, target, mid, until, depth + 1)\n",
    "        }\n",
    "        copy(src, target, from, mid, depth + 1)\n",
    "        right.join()\n",
    "      }\n",
    "    }\n",
    "    copy(ys, xs, 0, xs.length, 0)\n",
    "  }\n",
    "\n",
    "  val standardConfig = config(\n",
    "    Key.exec.minWarmupRuns -> 20,\n",
    "    Key.exec.maxWarmupRuns -> 60,\n",
    "    Key.exec.benchRuns -> 60,\n",
    "    Key.verbose -> true\n",
    "  ) withWarmer(new Warmer.Default)\n",
    "\n",
    "  def initialize(xs: Array[Int]) {\n",
    "    var i = 0\n",
    "    while (i < xs.length) {\n",
    "      xs(i) = i % 100\n",
    "      i += 1\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def main(args: Array[String]) {\n",
    "    val length = 10000000\n",
    "    val maxDepth = 7\n",
    "    val xs = new Array[Int](length)\n",
    "    val seqtime = standardConfig setUp {\n",
    "      _ => initialize(xs)\n",
    "    } measure {\n",
    "      quickSort(xs, 0, xs.length)\n",
    "    }\n",
    "    println(s\"sequential sum time: $seqtime ms\")\n",
    "\n",
    "    val partime = standardConfig setUp {\n",
    "      _ => initialize(xs)\n",
    "    } measure {\n",
    "      parMergeSort(xs, maxDepth)\n",
    "    }\n",
    "    println(s\"fork/join time: $partime ms\")\n",
    "    println(s\"speedup: ${seqtime / partime}\")\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting warmup.\n",
      "0. warmup run running time: 203.452077 (covNoGC: NaN, covGC: NaN)\n",
      "1. warmup run running time: 152.031795 (covNoGC: 0.2046, covGC: 0.2046)\n",
      "2. warmup run running time: 153.511797 (covNoGC: 0.1725, covGC: 0.1725)\n",
      "3. warmup run running time: 152.972406 (covNoGC: 0.1530, covGC: 0.1530)\n",
      "4. warmup run running time: 152.600775 (covNoGC: 0.1391, covGC: 0.1391)\n",
      "5. warmup run running time: 153.380845 (covNoGC: 0.1280, covGC: 0.1280)\n",
      "6. warmup run running time: 152.875741 (covNoGC: 0.1194, covGC: 0.1194)\n",
      "7. warmup run running time: 152.836866 (covNoGC: 0.1123, covGC: 0.1123)\n",
      "8. warmup run running time: 152.854421 (covNoGC: 0.1064, covGC: 0.1064)\n",
      "9. warmup run running time: 153.077365 (covNoGC: 0.1012, covGC: 0.1012)\n",
      "10. warmup run running time: 153.389643 (covNoGC: 0.0967, covGC: 0.0967)\n",
      "11. warmup run running time: 153.32059 (covNoGC: 0.0927, covGC: 0.0927)\n",
      "12. warmup run running time: 153.460458 (covNoGC: 0.0892, covGC: 0.0892)\n",
      "13. warmup run running time: 154.060096 (covNoGC: 0.0859, covGC: 0.0859)\n",
      "14. warmup run running time: 153.056639 (covNoGC: 0.0831, covGC: 0.0831)\n",
      "15. warmup run running time: 153.749368 (covNoGC: 0.0805, covGC: 0.0805)\n",
      "16. warmup run running time: 153.142487 (covNoGC: 0.0782, covGC: 0.0782)\n",
      "17. warmup run running time: 154.09754 (covNoGC: 0.0760, covGC: 0.0760)\n",
      "18. warmup run running time: 153.092221 (covNoGC: 0.0741, covGC: 0.0741)\n",
      "19. warmup run running time: 153.834968 (covNoGC: 0.0722, covGC: 0.0722)\n",
      "Steady-state detected.\n",
      "Ending warmup.\n",
      "measurements: 149.112852, 148.999941, 149.009982, 149.250724, 148.990426, 149.07033, 148.953176, 149.110059, 149.138249, 149.130166, 150.100671, 149.033691, 150.01069, 149.013521, 149.402811, 149.339004, 149.280731, 149.489438, 158.77911, 179.128403, 173.712521, 160.143265, 149.243712, 149.356915, 149.23986, 150.177173, 152.17261, 150.351549, 150.256349, 148.99756, 149.367573, 149.147941, 149.120957, 149.292063, 150.207333, 149.755653, 150.111037, 149.262739, 149.130509, 149.100014, 149.642211, 151.053849, 149.256814, 149.303779, 149.31726, 149.30224, 148.911162, 150.155649, 148.868481, 149.275036, 148.982942, 148.852761, 149.086052, 149.826708, 148.994152, 149.013061, 149.283447, 149.430237, 149.065429, 156.389274\n",
      "sequential sum time: 150.77503086666667 ms\n",
      "Starting warmup.\n",
      "0. warmup run running time: 155.306902 (covNoGC: NaN, covGC: NaN)\n",
      "1. warmup run running time: 88.834325 (covNoGC: NaN, covGC: 0.3850)\n",
      "2. warmup run running time: 90.46546 (covNoGC: 0.0129, covGC: 0.3399)\n",
      "3. warmup run running time: 89.76843 (covNoGC: 0.0091, covGC: 0.3093)\n",
      "4. warmup run running time: 114.891925 (covNoGC: 0.0091, covGC: 0.2660)\n",
      "5. warmup run running time: 90.185068 (covNoGC: 0.0079, covGC: 0.2541)\n",
      "6. warmup run running time: 81.243409 (covNoGC: 0.0441, covGC: 0.2554)\n",
      "7. warmup run running time: 81.028559 (covNoGC: 0.0519, covGC: 0.2533)\n",
      "8. warmup run running time: 201.893569 (covNoGC: 0.0519, covGC: 0.3764)\n",
      "9. warmup run running time: 80.905707 (covNoGC: 0.0547, covGC: 0.3748)\n",
      "10. warmup run running time: 80.245434 (covNoGC: 0.0565, covGC: 0.3723)\n",
      "11. warmup run running time: 81.24948 (covNoGC: 0.0555, covGC: 0.3678)\n",
      "12. warmup run running time: 83.697 (covNoGC: 0.0526, covGC: 0.3612)\n",
      "13. warmup run running time: 92.743482 (covNoGC: 0.0526, covGC: 0.3499)\n",
      "14. warmup run running time: 81.067657 (covNoGC: 0.0518, covGC: 0.3455)\n",
      "15. warmup run running time: 102.120442 (covNoGC: 0.0767, covGC: 0.3334)\n",
      "16. warmup run running time: 101.467302 (covNoGC: 0.0878, covGC: 0.3225)\n",
      "17. warmup run running time: 105.393481 (covNoGC: 0.0998, covGC: 0.3121)\n",
      "18. warmup run running time: 113.571423 (covNoGC: 0.0998, covGC: 0.3028)\n",
      "19. warmup run running time: 81.531601 (covNoGC: 0.0988, covGC: 0.3007)\n",
      "20. warmup run running time: 80.918895 (covNoGC: 0.0980, covGC: 0.2838)\n",
      "21. warmup run running time: 119.957382 (covNoGC: 0.1279, covGC: 0.2838)\n",
      "22. warmup run running time: 135.347106 (covNoGC: 0.1686, covGC: 0.2891)\n",
      "23. warmup run running time: 135.25682 (covNoGC: 0.1915, covGC: 0.2918)\n",
      "24. warmup run running time: 118.097295 (covNoGC: 0.1915, covGC: 0.2921)\n",
      "25. warmup run running time: 88.853396 (covNoGC: 0.1873, covGC: 0.2926)\n",
      "26. warmup run running time: 79.385759 (covNoGC: 0.1912, covGC: 0.2936)\n",
      "27. warmup run running time: 80.933908 (covNoGC: 0.1944, covGC: 0.2936)\n",
      "28. warmup run running time: 82.131957 (covNoGC: 0.1969, covGC: 0.1977)\n",
      "29. warmup run running time: 85.328326 (covNoGC: 0.1981, covGC: 0.1955)\n",
      "30. warmup run running time: 100.280065 (covNoGC: 0.1981, covGC: 0.1896)\n",
      "31. warmup run running time: 82.280356 (covNoGC: 0.1977, covGC: 0.1891)\n",
      "32. warmup run running time: 80.666495 (covNoGC: 0.1978, covGC: 0.1907)\n",
      "33. warmup run running time: 82.759766 (covNoGC: 0.1970, covGC: 0.1944)\n",
      "34. warmup run running time: 80.928321 (covNoGC: 0.1967, covGC: 0.1945)\n",
      "35. warmup run running time: 80.161488 (covNoGC: 0.1972, covGC: 0.2000)\n",
      "36. warmup run running time: 80.936522 (covNoGC: 0.1984, covGC: 0.2046)\n",
      "37. warmup run running time: 82.891173 (covNoGC: 0.1976, covGC: 0.2070)\n",
      "38. warmup run running time: 92.221017 (covNoGC: 0.1976, covGC: 0.2032)\n",
      "39. warmup run running time: 99.716247 (covNoGC: 0.1972, covGC: 0.1999)\n",
      "40. warmup run running time: 98.594127 (covNoGC: 0.1968, covGC: 0.1958)\n",
      "41. warmup run running time: 98.19636 (covNoGC: 0.1953, covGC: 0.1877)\n",
      "42. warmup run running time: 98.589582 (covNoGC: 0.1923, covGC: 0.1589)\n",
      "43. warmup run running time: 82.990761 (covNoGC: 0.1914, covGC: 0.1161)\n",
      "44. warmup run running time: 84.550358 (covNoGC: 0.1828, covGC: 0.0883)\n",
      "Steady-state detected.\n",
      "Ending warmup.\n",
      "measurements: 78.146114, 79.724809, 77.858236, 77.437872, 86.950435, 74.196897, 78.030279, 76.430655, 73.973272, 75.278726, 78.226565, 76.848203, 77.622988, 79.386964, 75.827804, 76.709853, 84.163931, 74.680426, 80.046829, 77.47295, 75.137239, 75.087929, 73.414557, 78.424379, 76.166555, 75.63141, 76.291277, 84.709885, 79.323918, 76.300351, 77.839004, 75.489911, 73.968235, 77.469233, 79.246105, 79.47027, 79.083366, 77.261948, 87.733109, 77.592999, 78.984272, 74.370713, 79.606601, 76.003772, 76.199078, 76.685514, 77.323885, 77.380764, 87.906377, 76.307006, 75.678572, 74.19182, 78.10568, 75.048142, 74.08987, 75.332756, 75.342076, 76.400047, 84.178461, 88.954681\n",
      "fork/join time: 77.87909291666664 ms\n",
      "speedup: 1.9360142140844045\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MergeSort.main(Array(\"start\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Collections\n",
    "\n",
    "### Choice of Data Structure:\n",
    "\n",
    "* (Cons) **List** is not suitable because of linear time search and concatenation\n",
    "* *Alternatives*: **Array** and **Tree**\n",
    "\n",
    "```scala\n",
    "// sequential map for Array\n",
    "def mapASegSeq[A, B](inp: Array[A], left: Int, right: Int, f: A => B, out: Array[B]) = {\n",
    "    var i = left\n",
    "    while(i < right) {\n",
    "        out(i) = f(inp(i))\n",
    "        i += 1\n",
    "    }\n",
    "}\n",
    "\n",
    "// paralle map for Array\n",
    "def mapASegPar[A, B](inp: Array[A], left: Int, right: Int, f: A => B, out: Array[B] = {\n",
    "    if (right - left < threshold)\n",
    "        // use sequential for effciency\n",
    "        mapASegSeq(inp, left, right, f, out)\n",
    "    else {\n",
    "        val mid = left + (right - left) / 2\n",
    "        parallel(mapASegPar(inp, left, mid, f, out), mapASegPar(inp, mid, right, f, out)) \n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In fact, parallelization pays off but manually reimplementing all these operations instead using higher-order functions doesn't and is not elegant!\n",
    "\n",
    "## Array vs. (Immutable) Tree:\n",
    "\n",
    "### Array:\n",
    "\n",
    "* *Advantages*:\n",
    "\n",
    "    1. Random access to elements on shared memory can share array\n",
    "    2. Memory locality\n",
    "\n",
    "\n",
    "* *Diadvantages*:\n",
    "\n",
    "    1. Must ensure parallel tasks write to disjoint parts\n",
    "    2. Expensive concatenation\n",
    "\n",
    "### Immutable Tree:\n",
    "\n",
    "* *Advantages*:\n",
    "\n",
    "    1. Purely functional, create new one and keep old one\n",
    "    2. Disjointness of parallel tasks happens readily\n",
    "    3. Efficient to combine two Trees\n",
    "\n",
    "* *Disadvantages*:\n",
    "\n",
    "    1. High memory location overhead\n",
    "    2. Bad memory locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Fold (Reduce):\n",
    "\n",
    "* Need *associative operation* i.e. paratheses (orders of evaluations) don't matter! (Addition is associative while subtraction is not).\n",
    "\n",
    "* Build trees for expressions where leaves are values and nodes are the operations.\n",
    "\n",
    "```scala\n",
    "sealed abstract class Tree[A]\n",
    "case class Leaf[A](value: A) extends Tree[A]\n",
    "case class Node[A](left: Tree[A], right: Tree[A]) extends Tree[A]\n",
    "```\n",
    "### Sequential Reduce for Tree:\n",
    "\n",
    "```scala\n",
    "def reduce[A](t: Tree[A], f: (A, A) => A): A = t match {\n",
    "    case Leaf(v) => v\n",
    "    case Node(l, r) => f(reduce(l, f), reduce(r, f))\n",
    "}\n",
    "```\n",
    "\n",
    "### Parallel Reduce for Tree:\n",
    "\n",
    "```scala\n",
    "def reduceTreePar[A](t: Tree[A], f: (A, A) => A): A = t match {\n",
    "    case Leaf(v) => f(v)\n",
    "    case Node(l, r) => {\n",
    "        val (lv, rv) = parallel(reduceTreePar(l, f), reduceTreePar(r, f))\n",
    "        f(lv, rv)\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "NOTE: [Tree ratotions](https://en.wikipedia.org/wiki/Tree_rotation) are simply parantheses jaxtapositions.\n",
    "\n",
    "### Parallel Reduce for Array:\n",
    "\n",
    "* Assuming associative operation, we can choose *any* tree preserving the order of elements of the original Array (collection), such as balanced tree, etc.\n",
    "\n",
    "* Advantage: replace `Node` constructor with our associative operation $f,$ instead of directly building the tree\n",
    "* For small Arrays, use sequential reduce.\n",
    "\n",
    "```scala\n",
    "def reduceSegPar[A](inp: Array[A], left: A, right: A, f: (A, A) => A): A = {\n",
    "    if (right - left < threshold){\n",
    "        var res = inp(left)\n",
    "        var i = left+1\n",
    "        while (i < right) {\n",
    "            res = f(res, i+1)\n",
    "            i += 1\n",
    "        }\n",
    "    }\n",
    "    else {\n",
    "        val mid = left + (right - left) / 2\n",
    "        val (a1, a2) = parallel(reduceSegPar(inp, left, mid, f), reduceSegPar(inp, mid, right, f))\n",
    "        f(a1, a2)\n",
    "    }\n",
    "}\n",
    "\n",
    "def reducePar[A](inp: Array[A], f: (A, A) => A): A = reduceSegPar[A](inp, 0, inp.length, f)\n",
    "```\n",
    "\n",
    "NOTE: Even in sequential case, we need accosiativity to guarantee that the results of `reduce, reduceLeft` and `reduceRight` agree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associative vs. Commutative:\n",
    "\n",
    "* Neither implies the other!\n",
    "* Floating point addition and multiplication are commutative but not accosiative (machine percision flaw!)\n",
    "\n",
    "### Make an operation commutative (if there's ordering on the input)\n",
    "```scala\n",
    "def f(x: A, y: A) = if (less(y, x)) g(y, x) else g(x, y)\n",
    "```\n",
    "\n",
    "### Create accosiative function out of two accosiative functions:\n",
    "\n",
    "Let $f_1: (A_1, A_1) => A_1, f_2: (A_2, A_2) => A_2$ be accosiative, then the function $f: \\left((A_1, A_2),(A_1, A_2)\\right) => (A_1, A_2)$ mapping \n",
    "\n",
    "$$f \\left((x_1, x_2),(y_1, y_2) \\right) = (f_1(x_1, y_1), f_2(x_2, y_2))$$ \n",
    "\n",
    "is associative.\n",
    "\n",
    "NOTE: So map-reduce is possible! for example, computing average `val res = reduce(map(collection, (x: Int) => (x, 1)), _+_)` then `val average = res._1/res._2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parallel Scan (Prefix Sum):\n",
    "\n",
    "### Sequential Scan:\n",
    "\n",
    "* $\\text{List}(a_1, \\cdots , a_n).\\text{scanLeft}(a_0)(f) = \\text{List}(b_0, \\cdots, b_n)$\n",
    "\n",
    "where $f$ is an *associative* function, $b_0 = a_0$ and $b_i = f(b_{i-1}, a_i).$\n",
    "\n",
    "```scala\n",
    "scanLeftSeq[A](inp: Array[A], a0: A, f: (A, A) => A, out: Array[A]): Unit = {\n",
    "    out(0) = a0\n",
    "    var i = 0\n",
    "    var a = a0\n",
    "    while(i < inp.length) {\n",
    "        a = f(a, inp(i))\n",
    "        i += 1\n",
    "        out(i) = a\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Parallel Scan:\n",
    "\n",
    "For simplicity, assume our initial input is given as a *tree*. So for input and result, we have\n",
    "\n",
    "```scala\n",
    "sealed abstract class Tree[A]\n",
    "case class Leaf[A](a: A) extends Tree[A]\n",
    "case class Node[A](l: Tree[A], r: Tree[A]) extends Tree[A]\n",
    "\n",
    "sealed abstract class TreeRes[A] {\n",
    "    val res: A\n",
    "}\n",
    "case class LeafRes[A](override val res: A) extends TreeRes[A] // res is now a field as well as a constructor\n",
    "class NodeRes[A](l: TreeRes[A], override val res: A, r: TreeRes[A]) extends TreeRes[A]\n",
    "```\n",
    "* **Step 1:** *Going up* in parallel i.e. parallel reduce (upsweep)\n",
    "\n",
    "```scala\n",
    "// bottom up \n",
    "def unsweep[A](t: Tree[A], f: (A, A) => A): TreeRes[A] = t match { // using case class in our advantage\n",
    "    case Leaf(v) => LeafRes(f(v))\n",
    "    case Node(l, r) => {\n",
    "        val (tL, tR) = parallel(unsweep(l, f), unsweep(r, f))\n",
    "        reduceRes(tL, f(tL.res, tR.res), tR)\n",
    "    }\n",
    "}\n",
    "```\n",
    "* **Step 2:** *Going down* in parallel\n",
    "\n",
    "```scala\n",
    "// 'a0' is the reduce of all elements left to tree 't'\n",
    "def downsweep[A](t: TreeRes[A], a0: A, f: (A, A) => A): Tree[A] = t match {\n",
    "    case LeafRes(a) => Leaf(f(a0, a))\n",
    "    case NodeRes(l, _, r) => {\n",
    "        val (tL, tR) = parallel(downsweep(l, a0, f), downsweep(r, f(a0, l.res), f))\n",
    "        Node(tL, tR)\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Therefore,\n",
    "\n",
    "```scala\n",
    "def scanLeftPar[A](t: Tree[A], a0: A, f: (A, A) => A): Tree[A] = {\n",
    "    val tRes = upsweep(t, f) // tree of results\n",
    "    val scl = downsweep(tRes, a0, f)\n",
    "    prepend(a0, scl)\n",
    "}\n",
    "\n",
    "// not worry about balancing\n",
    "def prepend[A](x: A, t: Tree[A]): Tree[A] = t match {\n",
    "    case Leaf(v) => Node(Leaf(x), Leaf(v))\n",
    "    case Node(l, r) => Node(prepend(x, l), r)\n",
    "}\n",
    "```\n",
    "* For an approximately balanced tree the parallel scan is of $O(\\log n)$ (with enough resources) where $n$ is the number of elements in the initial collection (above is number of leaves plus nodes).\n",
    "\n",
    "* For parallel scan on an *Array*, the process will be very similar (need to store indices/pointer to the begin and end of an array segment when computing intermediate results) except, we perform the computation sequentially when our array is small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
